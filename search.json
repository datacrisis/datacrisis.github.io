[
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Not Research",
    "section": "",
    "text": "For all things non-research related; also serving as the \"others\" bin of projects."
  },
  {
    "objectID": "project.html#leave-your-clothes-behind---lycb",
    "href": "project.html#leave-your-clothes-behind---lycb",
    "title": "Not Research",
    "section": "1 Leave Your Clothes Behind - LYCB",
    "text": "1 Leave Your Clothes Behind - LYCB\n\n\n\nVisualization of each stages of LYCB\n\n\nAbstract The demand for assets in the virtual world as recently gained a lot of attention. We present a novel framework dubbed LYCB: Leave Your Clothes Behind that allows users to directly generate a 3D mesh object of garments from a monocular video using Neural Radiance fields. The proposed method fills the gap in literature by addressing issues such as accurately fitting on to foreign bodies for virtual try-ons and model complex cloth properties using Neural Radiance fields and physics-based simulations. This lets us transfer clothing type and bodies easily.\nGithub Repository https://github.com/IamShubhamGupto/LYCB\nCommentary A project for ECE-GY 7123 Deep Learning, done together with Shubham Gupta.\nBasically, we were trying to extract a 3D mesh of arbitrary clothing from a given 2D monocular input sequence (e.g.¬†video). Similar approaches exist such as Structured Local Radiance Fields for Human Avatar Modeling where they tried to create clothed humans from monocular inputs in one-shot, and SCARF where they tried to implicitly disentangle the human body and garment meshes; both methods rely on implicit methods to render clothing deformations for each frame.\n\n\nFeng, Yao, et al.¬†‚ÄúCapturing and animation of body and clothing from monocular video.‚Äù SIGGRAPH Asia 2022 Conference Papers. 2022.\n\nZheng, Zerong, et al.¬†‚ÄúStructured local radiance fields for human avatar modeling.‚Äù Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n\nThe key differentiation of LYCB is to instead rely on battle-tested clothing physics simulator (e.g.¬†Blender) to perform the deformation instead! The motivation behind this approach is simple - in order for virtual try-on applications to leverage such approaches for real-world scenarios, accurate fitting and clothing deformation will be vital! (Wouldn‚Äôt want to have a new piece of clothing arrive and end up having to return it because it simply does not fit as shown prior üò¢).\nObviously, our initial attempt (that was completed in ~3 weeks from end-to-end) is very rough still. For example, the rendered mesh is not very clean and inherits unwanted deformation from the input footage; key takeaway is that there are much to be improved upon still!\nRegardless, we believe that such hybrid approaches of blending heuristic simulators for deformation + deep implicit methods would be a viable approach to build a virtual try-on platform where fit accuracy and consistency would be key (at least in the short term until we nail implicit clothing deformation physics!)."
  },
  {
    "objectID": "project.html#activations-and-augmentations-pushing-the-performance-of-isotropic-convnets---convmixerxl",
    "href": "project.html#activations-and-augmentations-pushing-the-performance-of-isotropic-convnets---convmixerxl",
    "title": "Not Research",
    "section": "2 Activations And Augmentations: Pushing The Performance Of Isotropic ConvNets - ConvMixerXL",
    "text": "2 Activations And Augmentations: Pushing The Performance Of Isotropic ConvNets - ConvMixerXL\n\n\n\nPipeline of ConvMixerXL - simpler a larger ConvMixer\n\n\nAbstract This project focuses on improving the performance of isotropic Convolutional Neural Networks (ConvNets) for computer vision tasks. Specifically, we experiment with training a ConvMixerXL model on the CIFAR-10 dataset. ConvMixerXL is an extended version of the ConvMixer architecture, consisting of 66 layers and approximately 5 million parameters. To optimize the performance of ConvMixerXL, we conducted an ablation study where we explored different configurations of the architecture, as well as various augmentations and activation functions. Through our experiments, we found that applying augmentations and using the Swish (SiLU) activation function for deeper models yielded the best results. We achieved a top-1 accuracy of 94.52%.\nGithub Repository https://github.com/datacrisis/ConvMixerXL\nCommentary This project was undertaken as part of the midterm project course ECE-GY 7123 Deep Learning, done together with Shubham Gupta. The primary objective is to improve upon the state-of-the-art isotropic ConvMixer architecture (published in the paper: Patches Are All You Need?) for multiclass classification tasks.\n\n\nTrockman, Asher, and J. Zico Kolter. ‚ÄúPatches are all you need?.‚Äù arXiv preprint arXiv:2201.09792 (2022).\nIt‚Äôs nothing at all fancy, we had to work in accordance to the allowed tweaks and constraints outlined by the course instructor, which does not lend itself to a great degree of technical freedom. The focus was on fine-tuning and optimizing the vanilla ConvMixer architecture, rather than introducing novel or complex modifications.\nStill, one thing that I am somewhat happy with is that my team and I pulled off the entire project and report writing within a week (and bagged a perfect score)! It‚Äôs not much, but it‚Äôs honest work. ü•î"
  },
  {
    "objectID": "project.html#malaysias-first-machine-learning-hackathon-crash-course-for-teens---1stdayhack",
    "href": "project.html#malaysias-first-machine-learning-hackathon-crash-course-for-teens---1stdayhack",
    "title": "Not Research",
    "section": "3 Malaysia‚Äôs First Machine Learning Hackathon + Crash Course for Teens - 1stDayHack",
    "text": "3 Malaysia‚Äôs First Machine Learning Hackathon + Crash Course for Teens - 1stDayHack\n\n\n\nSample title slide from the 1stDayHack crash course (I think it‚Äôs pretty neat still üòé)\n\n\nAbstract As delineated in the title, Malaysia‚Äôs First Machine Learning Hackathon + Crash Course targeted at teens in Malaysia (though this is from quite awhile ago!). Pulled it off together with 2 of my wonder accomplices, Edmond Yap and Chee Seng Leong!\nAll in all, I would say it was a great success, as we were able to bring together 60+ teenagers from across Peninsular Malaysia to learn and create for one whole week. The hackathon went very well and the responses were amazing; though the students‚Äô submissions were even cooler, despite the the short time they have had to pick up Python and concepts of Machine Learning on the go! Couldn‚Äôt have been more proud of them üòé.\nGithub Repository https://github.com/1stDayHack\nNotes and Lesson Materials My embarrassing notes\nCommentary The motivation behind the initiative is basically me trying to get younger sister to learn about programming, Machine Learning concepts, and to provide her with a first hand experience with hackathons! Unfortunately, I‚Äôve found that no such initiatives exist at the time in Malaysia, so I got together with two of my trusty friends and did it ourselves üë©‚Äçüè´.\nI‚Äôm not really satisfied with the content and recording of the course (in fact, quite embarrassed by them still), as they are awfully boring, and are not as well structured and polished as I would like it to be.\nIn defense of past-me, past-me did do everything on his own for the course contents and materials - writing the FirstDayKit (FDK) library, writing the script for video lessons, creating written course materials (notes, tutorials, etc), recording all the lesson videos, and publishing the whole thing in ~1 month, mostly late in the evening after work in his bedroom, so the quality of the end result is not very good and contents are rough around the edges. However, I‚Äôm happy to report that he‚Äôs a much better (I think) presenter and developer now!\nRegardless, it has been immensely satisfying overall and I‚Äôll certainly be revisiting the project again in the future! Let‚Äôs aim for 200 participants this time?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Right Place",
    "section": "",
    "text": "Hello there! üëãüèª\nMy name is Keifer Lee, a MS student @ New York University for the moment.\nI‚Äôm originally from tropical Malaysia, where the people are varied and tasty foods are aplenty; while ü•Ø bagels are great, Malaysian cuisines will always have a special place in my heart!\nI also dabble in research as a Graduate Student Researcher at the AI4CE Lab @ New York University, working with Professor Chen Feng primarily on Robotic Perception and Novel View Synthesis. In the past, I had a blast working on Autonomous UAVs with Dr.¬†Swee King Phang at Taylor‚Äôs University as an undergraduate (good ol‚Äô times!). As a side quest üó°, I‚Äôve been exploring the realm of LLMs in my spare time.\nIn terms of professional work, I was a Data Scientist @ MoneyLion for two exciting years (&gt;$1B served annually) as part of the (amazing) Instacash team, and I had a blast this summer serving as a Data Science Intern at IBM‚Äôs Chief Analytics Office (go CAO! üá∫üá∏).\nI‚Äôll also be serving as a Teaching Assistant for the graduate Robot Perception (ROB-GY 6203) course for Fall 2023. If you‚Äôre taking the course, feel free to reach out!"
  },
  {
    "objectID": "index.html#chronological-milestones",
    "href": "index.html#chronological-milestones",
    "title": "The Right Place",
    "section": "Chronological Milestones",
    "text": "Chronological Milestones\nNote: If you are a time traveller, this may be of particular use to you.\n\nSeptember 2023:\n\nWill be serving as a Teaching Assistant for the Robot Perception (ROB-GY 6203) in Fall 2023.\n\nMay 2023:\n\nJoined IBM in New York as a Data Science Intern, serving in the Chief Analytics Office. (where some of the best people I‚Äôve ever met are at!)\n\nJanuary 2023:\n\nWorked as a Graduate Research Assistant (or a GSET in NYU-speak) on a Sim2Real project for an industrial partner with Professor Chen Feng and the AI4CE Lab.\n\nOctober 2022:\n\nJoined the AI4CE Lab as a graduate student researcher.\n\nAugust 2022:\n\nConcluded my tenureship at MoneyLion after 2 exciting years (my #1 team)!\nPacked my bags, hopped on a flight, and arrived at New York University to begin my graduate studies.\n\nJanuary 2021:\n\nOrganized and successfully completed 1stDayHack with over 60 middle schoolers from across Peninsula Malaysia! I‚Äôm still proud of all the participants for what they‚Äôve achieved!ü•á\n\nSeptember 2020:\n\nJoined MoneyLion as a Data Scientist on the Instacash analytics team.\n\nMarch 2020:\n\nCOVID-19 ü¶† and nationwide lockdown in Malaysia. Lots of self-reflection and pivoting happened during this period!\n\nSeptember 2019:\n\nKickstarted a new initiative to organize Malaysia‚Äôs first Machine Learning hackathon + online course for teens - 1stDayHack!\n\nAugust 2019:\n\nAwarded the Best High Impact Research Award @ 12th International Engineering Research Conference, Malaysia.\nGraduated and successfully registered as a Graduate Engineer with the Board of Engineers Malaysia.\nSomewhere along the way, I received the Dean‚Äôs List Award 5 times, and netted 2 Book Prize awards.\nJoined the Taylor‚Äôs Unmanned Aerial Vehicle Lab as a Research Engineer working on Autonomous UAV with Dr.¬†Swee King Phang.\n\nMay 2019:\n\nJoined and won the Best Social Impact Award at the 2019 CodeathonX KL @ University Malaya.\n\nAugust 2018:\n\nJoined and won 1st runner-up at the 2018 NASA SpaceApp Challenge.\nAwarded the Best High Impact Research Award @ 11th International Engineering Research Conference, Malaysia.\n\nMarch 2015:\n\nStarted my life as a (B.Eng. in Electrical and Electronic Engineering) undergraduate @ Taylor‚Äôs University, Malaysia.\n\n13.7B BC:\n\nBeginning of time-ish."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "This section is embarrassingly sparse as of now, please bear with me as I fill it up. If you would like to collaborate, please feel free to reach out anytime and I‚Äôll try to revert back (almost) ASAP.\nMy primary research interest can be summarized in this one sentence - I want to research and develop robots to aid extraterrestrial exploration and colonization (e.g.¬†construction or exploration bots that will be widely use on Mars one day).\nWith the above idea in mind, the relevant tasks from an applicative point of view that I would like to tackle would be"
  },
  {
    "objectID": "research.html#principled-approach-towards-optimal-neural-radiance-field-learning-work-in-progress",
    "href": "research.html#principled-approach-towards-optimal-neural-radiance-field-learning-work-in-progress",
    "title": "Research",
    "section": "1 Principled Approach Towards Optimal Neural Radiance Field Learning (Work in Progress)",
    "text": "1 Principled Approach Towards Optimal Neural Radiance Field Learning (Work in Progress)\nAn on-going research project I‚Äôm co-leading at the AI4CE Lab as of now. The title above is simply my own interpretation of a working title and will likely be subject to changes.\nCan‚Äôt say much more for now, so stay tuned!"
  },
  {
    "objectID": "research.html#sim2real-for-robotic-manipulation",
    "href": "research.html#sim2real-for-robotic-manipulation",
    "title": "Research",
    "section": "2 Sim2Real for Robotic Manipulation",
    "text": "2 Sim2Real for Robotic Manipulation\nA collaboration project between the AI4CE Lab and an industry partner working in robotic systems and their applications. I couldn‚Äôt say much since the actual project details are confidential unfortunately. However, we did manage to improve the baseline model‚Äôs performance on key metrics tremendously via contrastive self-supervise approaches.\nSo instead, look at this funny GIF I found.\n\n\n\n\n\nI couldn‚Äôt find the original source, so this is the best I can do for now - https://bostondynamics.com and https://i.imgur.com/YnrWZsL.gifv"
  },
  {
    "objectID": "research.html#monocular-target-tracking-for-autonomous-uav",
    "href": "research.html#monocular-target-tracking-for-autonomous-uav",
    "title": "Research",
    "section": "3 Monocular Target Tracking for Autonomous UAV",
    "text": "3 Monocular Target Tracking for Autonomous UAV\n\n\n\nOur test drone, Hornet-1 doing a fly-by to say ‚ÄúHello!‚Äù\n\n\nAbstract My undergraduate project with the Taylor‚Äôs Unmanned Aerial Vehicle Lab, Dr.¬†Swee King Phang and Jun Jet Tai. The key objective is to develop an autonomous UAV capable of consistently track a given target during initialization. My main contributions are the development of a light-weight single target object-tracking algorithm that would be able of running at operational refresh rate (e.g.¬†&gt;30 FPS) on the light-weight single board computer."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Keifer Lee",
    "section": "",
    "text": "Email\n\n\nkl3866 [at] nyu [dot] edu\n\n \n\nLinkedIn\n\n\nhttps://www.linkedin.com/in/keiferlee/\n\n \n\nAddress\n\n\n6 MetroTech Center Brooklyn, NY 11201, USA"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Keifer Lee",
    "section": "",
    "text": "Here is a copy of my public CV. For more information, feel free to reach out to me and I‚Äôll be happy to chat ‚òïÔ∏è"
  }
]